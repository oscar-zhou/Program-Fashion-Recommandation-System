{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/torch8019/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets,transforms\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import random\n",
    "import cv2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "切訓練 測試 \n",
    "\n",
    "train/fit 2400 train/nofit 2375\n",
    "\n",
    "test/fit 600 test/nofit 610"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.DS_Store', 'a', 'c', 'd', 'b']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"../p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 6, 8]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform=transforms.Compose([\n",
    "   transforms.CenterCrop([560,450]),\n",
    "    transforms.Resize([224,224]),\n",
    "])\n",
    "\n",
    "os.mkdir(\"train\")\n",
    "os.mkdir(\"test\")\n",
    "os.mkdir(\"train/fit\")\n",
    "os.mkdir(\"test/fit\")\n",
    "os.mkdir(\"train/nofit\")\n",
    "os.mkdir(\"test/nofit\")\n",
    "\n",
    "train_fit_sets = random.sample(os.listdir(\"OB outfit sets/fit\"), 3000)\n",
    "test_fit_sets = random.sample(train_fit_sets, 600)\n",
    "train_nofit_sets = random.sample(os.listdir(\"OB outfit sets/nofit\"), 2985)\n",
    "test_nofit_sets = random.sample(train_nofit_sets, 610)\n",
    "\n",
    "for x in test_fit_sets:\n",
    "    train_fit_sets.remove(x)\n",
    "for x in test_nofit_sets:\n",
    "    train_nofit_sets.remove(x)\n",
    "    \n",
    "ct = 0\n",
    "for fit_set in train_fit_sets:\n",
    "    for f_item in os.listdir(f'OB outfit sets/fit/{fit_set}'):\n",
    "        path = \"OB outfit sets/fit/\"+fit_set+\"/\"+f_item\n",
    "        pil_img=Image.open(path)\n",
    "        t=transform(pil_img)\n",
    "        os.mkdir(\"train/fit/fit\"+ct)\n",
    "        save_dir=\"train/fit/fit\"+ct+\"/\"+f_item\n",
    "        ct += 1\n",
    "        t.save(save_dir)\n",
    "ct = 0\n",
    "for fit_set in test_fit_sets:\n",
    "    for f_item in os.listdir(f'OB outfit sets/fit/{fit_set}'):\n",
    "        path = \"OB outfit sets/fit/\"+fit_set+\"/\"+f_item\n",
    "        pil_img=Image.open(path)\n",
    "        t=transform(pil_img)\n",
    "        os.mkdir(\"test/fit/fit\"+ct)\n",
    "        save_dir=\"test/fit/fit\"+ct+\"/\"+f_item\n",
    "        ct += 1\n",
    "        t.save(save_dir)\n",
    "ct = 0\n",
    "for nofit_set in train_nofit_sets:\n",
    "    for nof_item in os.listdir(f'OB outfit sets/nofit/{nofit_set}'):\n",
    "        path = \"OB outfit sets/fit/\"+nofit_set+\"/\"+nof_item\n",
    "        pil_img=Image.open(path)\n",
    "        t=transform(pil_img)\n",
    "        os.mkdir(\"train/nofit/nofit\"+ct)\n",
    "        save_dir=\"train/nofit/nofit\"+ct+\"/\"+nof_item\n",
    "        ct += 1\n",
    "        t.save(save_dir)\n",
    "ct = 0\n",
    "for nofit_set in test_nofit_sets:\n",
    "    for nof_item in os.listdir(f'OB outfit sets/nofit/{nofit_set}'):\n",
    "        path = \"OB outfit sets/fit/\"+nofit_set+\"/\"+nof_item\n",
    "        pil_img=Image.open(path)\n",
    "        t=transform(pil_img)\n",
    "        os.mkdir(\"test/nofit/nofit\"+ct)\n",
    "        save_dir=\"test/nofit/nofit\"+ct+\"/\"+nof_item\n",
    "        ct += 1\n",
    "        t.save(save_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loss function參考"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss function參考1\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        # Load VGG16 model as the backbone\n",
    "        self.backbone = models.vgg16(pretrained=True)\n",
    "        # Remove the last layer (the classification layer) of VGG16\n",
    "        self.backbone.classifier = nn.Sequential(*list(self.backbone.classifier.children())[:-1])\n",
    "        # Add a fully connected layer with 512 output units for Siamese network\n",
    "        self.fc = nn.Linear(512, 1)\n",
    "\n",
    "    def forward_once(self, x):\n",
    "        # Pass input image through the VGG16 backbone\n",
    "        output = self.backbone(x)\n",
    "        # Flatten the output\n",
    "        output = output.view(output.size()[0], -1)\n",
    "        # Pass through the fully connected layer\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        # Pass the first image through the network\n",
    "        output1 = self.forward_once(x1)\n",
    "        # Pass the second image through the network\n",
    "        output2 = self.forward_once(x2)\n",
    "        # Return the absolute difference between the two outputs\n",
    "        distance = torch.abs(output1 - output2)\n",
    "        return distance\n",
    "\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, distance, label):\n",
    "        # Compute the contrastive loss\n",
    "        loss = torch.mean((1 - label) * torch.pow(distance, 2) +\n",
    "                          (label) * torch.pow(torch.clamp(self.margin - distance, min=0.0), 2))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss function參考2\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, target):\n",
    "        euclidean_distance = nn.functional.pairwise_distance(output1, output2)\n",
    "        loss_contrastive = torch.mean((1 - target) * torch.pow(euclidean_distance, 2) +\n",
    "                                      target * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
    "        return loss_contrastive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss function參考3\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Contrastive loss\n",
    "    Takes embeddings of two samples and a target label == 1 if samples are from the same class and label == 0 otherwise\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, margin):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.eps = 1e-9\n",
    "\n",
    "    def forward(self, output1, output2, target, size_average=True):\n",
    "        distances = (output2 - output1).pow(2).sum(1)  # squared distances\n",
    "        losses = 0.5 * (target.float() * distances +\n",
    "                        (1 + -1 * target).float() * F.relu(self.margin - (distances + self.eps).sqrt()).pow(2))\n",
    "        return losses.mean() if size_average else losses.sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define dataset\n",
    "#data_dir應該要擺成train/fit/data_pair/2張圖片\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize([224,224]),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[.5,.5,.5],std=[.5,.5,.5])\n",
    "])\n",
    "batch_size = 64\n",
    "train_dir = \"檔案位置\"           \n",
    "#定義數據集\n",
    "train_datasets = datasets.ImageFolder(train_dir, transform=transform)\n",
    "#加載數據集\n",
    "train_dataloader = torch.utils.data.DataLoader(train_datasets, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# val_dir = \"test\"\t\t\n",
    "# val_datasets = datasets.ImageFolder(val_dir, transform=transform2)\n",
    "# val_dataloader = torch.utils.data.DataLoader(val_datasets, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['train/fit/pair2/fit377.png', 'train/fit/pair2/fit376.png', 0],\n",
       " ['train/fit/pair1/fit374.png', 'train/fit/pair1/fit375.png', 0],\n",
       " ['train/nofit/pair1/fit381.png', 'train/nofit/pair1/fit382.png', 1],\n",
       " ['train/nofit/pair2/fit384.png', 'train/nofit/pair2/fit383.png', 1]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#define data loader\n",
    "train_root_dir = 'train'\n",
    "train_pairs_list = []\n",
    "\n",
    "for fit_ in ['fit', 'nofit']:\n",
    "    fit_dir = f\"{train_root_dir}/{fit_}\"\n",
    "    for pair in os.listdir(fit_dir)[1:]:\n",
    "        imgs = os.listdir(f\"{fit_dir}/{pair}\")[1:]\n",
    "        train_pairs_list.append([\n",
    "                            f\"{fit_dir}/{pair}/{imgs[0]}\", \n",
    "                            f\"{fit_dir}/{pair}/{imgs[1]}\",\n",
    "                            int(fit_ == 'fit')\n",
    "                            ])\n",
    "        # train_pairs_list_label.append(int(fit_ != 'fit'))\n",
    "        # for img in imgs:\n",
    "        #     train_pairs_list.append(f\"{fit_dir}/{img}\")\n",
    "        #     train_pairs_list_label.append(int(fit_ != 'fit'))\n",
    "#print(*train_pairs_list, sep = '\\n')\n",
    "random.shuffle(train_pairs_list)\n",
    "train_pairs_list\n",
    "# for i, j in zip(train_pairs_list, train_pairs_list_label):\n",
    "#     print(i, j)\n",
    "# print(*train_pairs_list_label, sep = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pairs_loader(train_data, batch_size):\n",
    "    cnt = len(train_data) // batch_size + int(len(train_data) % batch_size != 0)\n",
    "    for i in range(cnt):\n",
    "        temp = i * (batch_size)\n",
    "        yield train_data[temp : temp + batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['train/fit/pair1/fit374.png', 'train/fit/pair1/fit375.png', 0], ['train/fit/pair2/fit377.png', 'train/fit/pair2/fit376.png', 0]]\n",
      "[['train/nofit/pair1/fit381.png', 'train/nofit/pair1/fit382.png', 1], ['train/nofit/pair2/fit384.png', 'train/nofit/pair2/fit383.png', 1]]\n"
     ]
    }
   ],
   "source": [
    "# x = my_imgs_pair_loader(list(range(10)), 6)\n",
    "x = train_pairs_loader(train_pairs_list, 2)\n",
    "for i in x:\n",
    "    print(i)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 胡亂測試"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@823.664] global /Users/runner/miniforge3/conda-bld/libopencv_1661642967242/work/modules/imgcodecs/src/loadsave.cpp (239) findDecoder imread_('train/nofit/fit384.png'): can't open/read file: check file path/integrity\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "pic should be Tensor or ndarray. Got <class 'NoneType'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/5b/gmxdy6dd0nb181cmtr2vfwrc0000gn/T/ipykernel_30365/492351277.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train/nofit/fit384.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# img = Image.open(\"train/nofit/pair2/fit384.png\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mimg_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mimg_t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/torch8019/lib/python3.7/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/torch8019/lib/python3.7/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \"\"\"\n\u001b[0;32m--> 226\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pil_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/torch8019/lib/python3.7/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_pil_image\u001b[0;34m(pic, mode)\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0m_log_api_usage_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_pil_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"pic should be Tensor or ndarray. Got {type(pic)}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: pic should be Tensor or ndarray. Got <class 'NoneType'>."
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize([224,224]),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[.5,.5,.5],std=[.5,.5,.5])\n",
    "])\n",
    "\n",
    "img = Image.open(\"train/nofit/pair2/fit384.png\")\n",
    "img_t = transform(img)\n",
    "img_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('train/fit/pair1/fit374.png',), ('train/fit/pair1/fit375.png',), tensor([0])]\n",
      "[('train/fit/pair2/fit377.png',), ('train/fit/pair2/fit376.png',), tensor([0])]\n",
      "[('train/nofit/pair1/fit381.png',), ('train/nofit/pair1/fit382.png',), tensor([1])]\n",
      "[('train/nofit/pair2/fit384.png',), ('train/nofit/pair2/fit383.png',), tensor([1])]\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize([224,224]),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[.5,.5,.5],std=[.5,.5,.5])\n",
    "])\n",
    "train_dataloader = torch.utils.data.DataLoader(pairs_list,batch_size=1)\n",
    "for i in train_dataloader:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[0.8118, 0.8118, 0.8118,  ..., 0.8118, 0.8118, 0.8039],\n",
      "         [0.8196, 0.8196, 0.8196,  ..., 0.8118, 0.8118, 0.8039],\n",
      "         [0.8196, 0.8275, 0.8196,  ..., 0.8118, 0.8118, 0.8039],\n",
      "         ...,\n",
      "         [0.7961, 0.7961, 0.7961,  ..., 0.7490, 0.7098, 0.6471],\n",
      "         [0.7961, 0.7961, 0.7961,  ..., 0.7412, 0.7020, 0.6314],\n",
      "         [0.7961, 0.7882, 0.7961,  ..., 0.7412, 0.6941, 0.6157]],\n",
      "\n",
      "        [[0.8039, 0.8039, 0.8039,  ..., 0.7882, 0.7882, 0.7882],\n",
      "         [0.8118, 0.8118, 0.8118,  ..., 0.7882, 0.7882, 0.7882],\n",
      "         [0.8118, 0.8118, 0.8118,  ..., 0.7882, 0.7882, 0.7882],\n",
      "         ...,\n",
      "         [0.7098, 0.7098, 0.7098,  ..., 0.6392, 0.6000, 0.5137],\n",
      "         [0.7098, 0.7098, 0.7098,  ..., 0.6392, 0.5843, 0.4902],\n",
      "         [0.7098, 0.7020, 0.7098,  ..., 0.6314, 0.5765, 0.4745]],\n",
      "\n",
      "        [[0.7882, 0.7882, 0.7882,  ..., 0.7490, 0.7490, 0.7490],\n",
      "         [0.7961, 0.7961, 0.7961,  ..., 0.7490, 0.7490, 0.7412],\n",
      "         [0.8039, 0.7961, 0.7961,  ..., 0.7490, 0.7490, 0.7490],\n",
      "         ...,\n",
      "         [0.6627, 0.6627, 0.6627,  ..., 0.5686, 0.5137, 0.4196],\n",
      "         [0.6627, 0.6627, 0.6627,  ..., 0.5608, 0.5059, 0.3961],\n",
      "         [0.6627, 0.6549, 0.6627,  ..., 0.5608, 0.4902, 0.3647]]]), 6)\n",
      "torch.Size([3, 224, 224])\n",
      "label是6\n"
     ]
    }
   ],
   "source": [
    "#測試\n",
    "print(train_datasets[5755])\n",
    "print(train_datasets[5755][0].shape)\n",
    "print(f'label是{train_datasets[5755][1]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "None (570, 760) RGB\n"
     ]
    }
   ],
   "source": [
    "#測試\n",
    "train_dir = \"../compatibility task/filtered data\"           \n",
    "train_datasets = datasets.ImageFolder(train_dir)\n",
    "img ,label= train_datasets[5755]\n",
    "print(label)\n",
    "print(img.format, img.size, img.mode)\n",
    "img.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "#測試\n",
    "train_dir = \"train\"           \n",
    "train_datasets = datasets.ImageFolder(train_dir)\n",
    "img ,label= train_datasets[4]\n",
    "print(label)\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#測試\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize([224,224]),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[.5,.5,.5],std=[.5,.5,.5])\n",
    "])\n",
    "train_dir = \"train\"   \n",
    "batch_size = 1\n",
    "train_datasets = datasets.ImageFolder(train_dir,transform=transform)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_datasets, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.8510,  0.8510,  0.8510,  ...,  0.4118,  0.2941,  0.3490],\n",
      "          [ 0.8510,  0.8510,  0.8510,  ...,  0.3961,  0.2941,  0.3647],\n",
      "          [ 0.8588,  0.8510,  0.8510,  ...,  0.3882,  0.3020,  0.3804],\n",
      "          ...,\n",
      "          [ 0.8039,  0.8118,  0.8118,  ...,  0.5137,  0.5216,  0.5137],\n",
      "          [ 0.8039,  0.8039,  0.8039,  ...,  0.5137,  0.5137,  0.5059],\n",
      "          [ 0.8039,  0.8039,  0.8118,  ...,  0.5059,  0.4980,  0.4980]],\n",
      "\n",
      "         [[ 0.7882,  0.7882,  0.7882,  ...,  0.0196, -0.3412, -0.3412],\n",
      "          [ 0.7882,  0.7882,  0.7882,  ...,  0.0039, -0.3333, -0.3255],\n",
      "          [ 0.7961,  0.7882,  0.7882,  ...,  0.0039, -0.3255, -0.3098],\n",
      "          ...,\n",
      "          [ 0.6863,  0.6784,  0.6784,  ...,  0.4039,  0.4118,  0.4196],\n",
      "          [ 0.6863,  0.6784,  0.6784,  ...,  0.4039,  0.4118,  0.4118],\n",
      "          [ 0.6784,  0.6784,  0.6784,  ...,  0.4039,  0.3961,  0.4039]],\n",
      "\n",
      "         [[ 0.7647,  0.7647,  0.7647,  ..., -0.2157, -0.6157, -0.6471],\n",
      "          [ 0.7647,  0.7647,  0.7647,  ..., -0.2314, -0.6235, -0.6235],\n",
      "          [ 0.7725,  0.7647,  0.7647,  ..., -0.2314, -0.6157, -0.6078],\n",
      "          ...,\n",
      "          [ 0.5529,  0.5451,  0.5529,  ...,  0.3255,  0.3255,  0.3333],\n",
      "          [ 0.5608,  0.5451,  0.5529,  ...,  0.3176,  0.3255,  0.3255],\n",
      "          [ 0.5529,  0.5529,  0.5529,  ...,  0.3176,  0.3255,  0.3333]]]]) tensor([0])\n",
      "tensor([[[[ 0.8588,  0.8588,  0.8588,  ...,  0.6706,  0.6863,  0.6863],\n",
      "          [ 0.8588,  0.8588,  0.8588,  ...,  0.6706,  0.6784,  0.6784],\n",
      "          [ 0.8588,  0.8588,  0.8588,  ...,  0.6941,  0.7020,  0.7098],\n",
      "          ...,\n",
      "          [ 0.5686,  0.5686,  0.5686,  ...,  0.1451,  0.1529,  0.1686],\n",
      "          [ 0.5608,  0.5608,  0.5686,  ...,  0.1451,  0.1451,  0.1529],\n",
      "          [ 0.5686,  0.5686,  0.5686,  ...,  0.1451,  0.1451,  0.1373]],\n",
      "\n",
      "         [[ 0.8196,  0.8196,  0.8196,  ..., -0.2471, -0.2549, -0.2549],\n",
      "          [ 0.8196,  0.8196,  0.8196,  ..., -0.2471, -0.2471, -0.2392],\n",
      "          [ 0.8196,  0.8196,  0.8196,  ..., -0.2157, -0.2078, -0.2157],\n",
      "          ...,\n",
      "          [ 0.6392,  0.6471,  0.6471,  ...,  0.0902,  0.0902,  0.0980],\n",
      "          [ 0.6314,  0.6392,  0.6471,  ...,  0.0902,  0.0902,  0.0980],\n",
      "          [ 0.6314,  0.6314,  0.6392,  ...,  0.0902,  0.0902,  0.0902]],\n",
      "\n",
      "         [[ 0.7882,  0.7882,  0.7882,  ..., -0.6706, -0.6863, -0.6941],\n",
      "          [ 0.7882,  0.7882,  0.7882,  ..., -0.6627, -0.6627, -0.6706],\n",
      "          [ 0.7882,  0.7882,  0.7882,  ..., -0.6392, -0.6314, -0.6392],\n",
      "          ...,\n",
      "          [ 0.6549,  0.6549,  0.6549,  ..., -0.0353, -0.0353, -0.0275],\n",
      "          [ 0.6471,  0.6471,  0.6549,  ..., -0.0353, -0.0353, -0.0275],\n",
      "          [ 0.6471,  0.6471,  0.6392,  ..., -0.0431, -0.0353, -0.0275]]]]) tensor([0])\n",
      "tensor([[[[ 0.8588,  0.8588,  0.8588,  ...,  0.6706,  0.6863,  0.6863],\n",
      "          [ 0.8588,  0.8588,  0.8588,  ...,  0.6706,  0.6784,  0.6784],\n",
      "          [ 0.8588,  0.8588,  0.8588,  ...,  0.6941,  0.7020,  0.7098],\n",
      "          ...,\n",
      "          [ 0.6471,  0.6549,  0.6471,  ...,  0.3725,  0.3961,  0.3020],\n",
      "          [ 0.6471,  0.6549,  0.6471,  ...,  0.3255,  0.3882,  0.3255],\n",
      "          [ 0.6471,  0.6471,  0.6471,  ...,  0.2471,  0.3725,  0.3255]],\n",
      "\n",
      "         [[ 0.8196,  0.8196,  0.8196,  ..., -0.2471, -0.2549, -0.2549],\n",
      "          [ 0.8196,  0.8196,  0.8196,  ..., -0.2471, -0.2471, -0.2392],\n",
      "          [ 0.8196,  0.8196,  0.8196,  ..., -0.2157, -0.2078, -0.2157],\n",
      "          ...,\n",
      "          [ 0.6392,  0.6471,  0.6392,  ...,  0.3255,  0.3569,  0.2314],\n",
      "          [ 0.6392,  0.6471,  0.6392,  ...,  0.2784,  0.3412,  0.2392],\n",
      "          [ 0.6392,  0.6392,  0.6392,  ...,  0.2157,  0.3333,  0.2549]],\n",
      "\n",
      "         [[ 0.7882,  0.7882,  0.7882,  ..., -0.6706, -0.6863, -0.6941],\n",
      "          [ 0.7882,  0.7882,  0.7882,  ..., -0.6627, -0.6627, -0.6706],\n",
      "          [ 0.7882,  0.7882,  0.7882,  ..., -0.6392, -0.6314, -0.6392],\n",
      "          ...,\n",
      "          [ 0.6078,  0.6157,  0.6078,  ...,  0.2314,  0.2784,  0.1451],\n",
      "          [ 0.6078,  0.6157,  0.6078,  ...,  0.1686,  0.2706,  0.1608],\n",
      "          [ 0.6078,  0.6078,  0.6078,  ...,  0.1059,  0.2627,  0.1686]]]]) tensor([0])\n",
      "tensor([[[[ 0.8588,  0.8588,  0.8588,  ...,  0.6706,  0.6863,  0.6863],\n",
      "          [ 0.8588,  0.8588,  0.8588,  ...,  0.6706,  0.6784,  0.6784],\n",
      "          [ 0.8588,  0.8588,  0.8588,  ...,  0.6941,  0.7020,  0.7098],\n",
      "          ...,\n",
      "          [ 0.9137,  0.9137,  0.9216,  ...,  0.7725,  0.7725,  0.7804],\n",
      "          [ 0.9216,  0.9216,  0.9294,  ...,  0.7725,  0.7725,  0.7804],\n",
      "          [ 0.9294,  0.9294,  0.9294,  ...,  0.7725,  0.7725,  0.7804]],\n",
      "\n",
      "         [[ 0.8196,  0.8196,  0.8196,  ..., -0.2471, -0.2549, -0.2549],\n",
      "          [ 0.8196,  0.8196,  0.8196,  ..., -0.2471, -0.2471, -0.2392],\n",
      "          [ 0.8196,  0.8196,  0.8196,  ..., -0.2157, -0.2078, -0.2157],\n",
      "          ...,\n",
      "          [ 0.9137,  0.9137,  0.9216,  ...,  0.6392,  0.6392,  0.6471],\n",
      "          [ 0.9216,  0.9216,  0.9294,  ...,  0.6392,  0.6392,  0.6471],\n",
      "          [ 0.9294,  0.9294,  0.9294,  ...,  0.6392,  0.6392,  0.6471]],\n",
      "\n",
      "         [[ 0.7882,  0.7882,  0.7882,  ..., -0.6706, -0.6863, -0.6941],\n",
      "          [ 0.7882,  0.7882,  0.7882,  ..., -0.6627, -0.6627, -0.6706],\n",
      "          [ 0.7882,  0.7882,  0.7882,  ..., -0.6392, -0.6314, -0.6392],\n",
      "          ...,\n",
      "          [ 0.9137,  0.9137,  0.9137,  ...,  0.5765,  0.5765,  0.5843],\n",
      "          [ 0.9216,  0.9216,  0.9294,  ...,  0.5765,  0.5765,  0.5843],\n",
      "          [ 0.9294,  0.9294,  0.9294,  ...,  0.5765,  0.5765,  0.5843]]]]) tensor([0])\n",
      "tensor([[[[-0.3961, -0.3882, -0.3804,  ..., -0.6235, -0.6549, -0.6706],\n",
      "          [-0.4039, -0.4039, -0.4039,  ..., -0.6471, -0.6706, -0.6941],\n",
      "          [-0.4510, -0.4510, -0.4431,  ..., -0.6863, -0.7020, -0.7176],\n",
      "          ...,\n",
      "          [ 0.6627,  0.6471,  0.6549,  ...,  0.2392,  0.2314,  0.2314],\n",
      "          [ 0.6549,  0.6627,  0.6627,  ...,  0.2314,  0.2392,  0.2314],\n",
      "          [ 0.6235,  0.6471,  0.6627,  ...,  0.2549,  0.2471,  0.2471]],\n",
      "\n",
      "         [[-0.6549, -0.6471, -0.6392,  ..., -0.8118, -0.8275, -0.8431],\n",
      "          [-0.6627, -0.6627, -0.6627,  ..., -0.8196, -0.8353, -0.8588],\n",
      "          [-0.7020, -0.6863, -0.6863,  ..., -0.8431, -0.8510, -0.8588],\n",
      "          ...,\n",
      "          [ 0.6392,  0.6157,  0.6235,  ...,  0.0902,  0.0745,  0.0667],\n",
      "          [ 0.6314,  0.6314,  0.6471,  ...,  0.0824,  0.0980,  0.0824],\n",
      "          [ 0.5843,  0.6235,  0.6471,  ...,  0.0980,  0.0980,  0.0980]],\n",
      "\n",
      "         [[-0.8039, -0.7961, -0.7569,  ..., -0.9216, -0.9294, -0.9451],\n",
      "          [-0.8118, -0.8039, -0.7961,  ..., -0.9294, -0.9451, -0.9608],\n",
      "          [-0.8510, -0.8431, -0.8353,  ..., -0.9451, -0.9608, -0.9686],\n",
      "          ...,\n",
      "          [ 0.6000,  0.5765,  0.5843,  ..., -0.0353, -0.0353, -0.0353],\n",
      "          [ 0.5922,  0.5922,  0.6078,  ..., -0.0275, -0.0275, -0.0353],\n",
      "          [ 0.5451,  0.5922,  0.6078,  ..., -0.0039, -0.0118, -0.0118]]]]) tensor([1])\n",
      "tensor([[[[-0.3961, -0.3882, -0.3804,  ..., -0.6235, -0.6549, -0.6706],\n",
      "          [-0.4039, -0.4039, -0.4039,  ..., -0.6471, -0.6706, -0.6941],\n",
      "          [-0.4510, -0.4510, -0.4431,  ..., -0.6863, -0.7020, -0.7176],\n",
      "          ...,\n",
      "          [-0.0588, -0.0902, -0.1765,  ..., -0.1686, -0.1608, -0.3569],\n",
      "          [ 0.0196,  0.0431,  0.0118,  ..., -0.2000, -0.3647, -0.7412],\n",
      "          [ 0.0275,  0.0196,  0.0196,  ...,  0.0196, -0.0980, -0.4745]],\n",
      "\n",
      "         [[-0.6549, -0.6471, -0.6392,  ..., -0.8118, -0.8275, -0.8431],\n",
      "          [-0.6627, -0.6627, -0.6627,  ..., -0.8196, -0.8353, -0.8588],\n",
      "          [-0.7020, -0.6863, -0.6863,  ..., -0.8431, -0.8510, -0.8588],\n",
      "          ...,\n",
      "          [ 0.1608,  0.1137,  0.0039,  ...,  0.0745,  0.0667, -0.1608],\n",
      "          [ 0.3098,  0.3098,  0.2863,  ...,  0.0196, -0.1843, -0.5843],\n",
      "          [ 0.3176,  0.3098,  0.3098,  ...,  0.2784,  0.1373, -0.2549]],\n",
      "\n",
      "         [[-0.8039, -0.7961, -0.7569,  ..., -0.9216, -0.9294, -0.9451],\n",
      "          [-0.8118, -0.8039, -0.7961,  ..., -0.9294, -0.9451, -0.9608],\n",
      "          [-0.8510, -0.8431, -0.8353,  ..., -0.9451, -0.9608, -0.9686],\n",
      "          ...,\n",
      "          [ 0.1686,  0.0902, -0.0039,  ...,  0.1608,  0.1451, -0.1216],\n",
      "          [ 0.3725,  0.3647,  0.3333,  ...,  0.0824, -0.1451, -0.6157],\n",
      "          [ 0.3882,  0.3882,  0.3804,  ...,  0.3882,  0.2235, -0.2157]]]]) tensor([1])\n",
      "tensor([[[[-0.0902, -0.2078, -0.4353,  ...,  0.4745, -0.1765, -0.5686],\n",
      "          [-0.1059, -0.1294, -0.5608,  ...,  0.4667, -0.1373, -0.5529],\n",
      "          [-0.1137, -0.1294, -0.4667,  ...,  0.4745, -0.0824, -0.5686],\n",
      "          ...,\n",
      "          [ 0.3804,  0.3647,  0.3569,  ...,  0.7255,  0.7176,  0.7098],\n",
      "          [ 0.3882,  0.3804,  0.3725,  ...,  0.7255,  0.7176,  0.7098],\n",
      "          [ 0.3804,  0.3804,  0.3882,  ...,  0.7333,  0.7020,  0.7098]],\n",
      "\n",
      "         [[ 0.3098,  0.1059, -0.6235,  ..., -0.0745, -0.5216, -0.7569],\n",
      "          [ 0.2941,  0.2706, -0.4510,  ..., -0.0824, -0.4902, -0.7412],\n",
      "          [ 0.2784,  0.2863, -0.2627,  ..., -0.0745, -0.4510, -0.7490],\n",
      "          ...,\n",
      "          [ 0.1686,  0.1451,  0.1373,  ...,  0.6392,  0.6157,  0.5843],\n",
      "          [ 0.1765,  0.1608,  0.1608,  ...,  0.6392,  0.6078,  0.5843],\n",
      "          [ 0.1686,  0.1686,  0.1765,  ...,  0.6314,  0.5922,  0.6000]],\n",
      "\n",
      "         [[-0.2000, -0.3647, -0.7647,  ..., -0.6627, -0.8275, -0.8902],\n",
      "          [-0.2157, -0.2549, -0.7490,  ..., -0.6627, -0.8118, -0.8824],\n",
      "          [-0.2314, -0.2392, -0.6549,  ..., -0.6549, -0.7882, -0.8902],\n",
      "          ...,\n",
      "          [ 0.0039, -0.0196, -0.0275,  ...,  0.5529,  0.5216,  0.4745],\n",
      "          [ 0.0118, -0.0039, -0.0039,  ...,  0.5529,  0.5059,  0.4824],\n",
      "          [ 0.0039,  0.0039,  0.0118,  ...,  0.5608,  0.4980,  0.4980]]]]) tensor([1])\n",
      "tensor([[[[-0.0902, -0.2078, -0.4353,  ...,  0.4745, -0.1765, -0.5686],\n",
      "          [-0.1059, -0.1294, -0.5608,  ...,  0.4667, -0.1373, -0.5529],\n",
      "          [-0.1137, -0.1294, -0.4667,  ...,  0.4745, -0.0824, -0.5686],\n",
      "          ...,\n",
      "          [ 0.0588,  0.0745,  0.0588,  ...,  0.3490,  0.3412,  0.3647],\n",
      "          [ 0.0353,  0.0431,  0.0353,  ...,  0.3647,  0.3412,  0.3020],\n",
      "          [ 0.0196,  0.0275,  0.0196,  ...,  0.4275,  0.4196,  0.3961]],\n",
      "\n",
      "         [[ 0.3098,  0.1059, -0.6235,  ..., -0.0745, -0.5216, -0.7569],\n",
      "          [ 0.2941,  0.2706, -0.4510,  ..., -0.0824, -0.4902, -0.7412],\n",
      "          [ 0.2784,  0.2863, -0.2627,  ..., -0.0745, -0.4510, -0.7490],\n",
      "          ...,\n",
      "          [-0.3961, -0.3882, -0.4039,  ...,  0.0667,  0.0667,  0.0902],\n",
      "          [-0.4039, -0.4039, -0.4118,  ...,  0.0824,  0.0588,  0.0196],\n",
      "          [-0.4196, -0.4196, -0.4196,  ...,  0.1294,  0.1294,  0.0980]],\n",
      "\n",
      "         [[-0.2000, -0.3647, -0.7647,  ..., -0.6627, -0.8275, -0.8902],\n",
      "          [-0.2157, -0.2549, -0.7490,  ..., -0.6627, -0.8118, -0.8824],\n",
      "          [-0.2314, -0.2392, -0.6549,  ..., -0.6549, -0.7882, -0.8902],\n",
      "          ...,\n",
      "          [-0.6157, -0.6157, -0.6235,  ..., -0.1765, -0.1765, -0.1608],\n",
      "          [-0.6314, -0.6314, -0.6314,  ..., -0.1686, -0.1843, -0.2078],\n",
      "          [-0.6471, -0.6314, -0.6314,  ..., -0.1451, -0.1529, -0.1686]]]]) tensor([1])\n"
     ]
    }
   ],
   "source": [
    "#測試\n",
    "for i in train_dataloader:\n",
    "    print(i[0],i[1])\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定義模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define embeddingnet which is used to extract image emabedding \n",
    "class EmbeddingNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EmbeddingNet, self).__init__()\n",
    "        self.features = models.vgg16(pretrained=True)\n",
    "        # ------------------------------------------------------------------------------\n",
    "        # Remove the last layer (the classification layer) of VGG16 第一種寫法 把原本VGG的最後一層fc layer刪掉 直接線性輸出\n",
    "        # self.backbone.classifier = nn.Sequential(*list(self.backbone.classifier.children())[:-1])\n",
    "        # self.features = self.backbone.classifier\n",
    "        # self.fc = nn.Linear(512, 1)\n",
    "        # ------------------------------------------------------------------------------\n",
    "        # 第二種寫法 \n",
    "        self.features.classifier = nn.Sequential()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512*7*7, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(128, 10),#原num_classes 這裡要用多少維輸出來代表這個pic\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        #經過VGG提取特徵\n",
    "        output = self.backbone(x)\n",
    "        #flatten\n",
    "        output = output.view(output.size(0)*output.size(1))\n",
    "        #經過fc層\n",
    "        output = self.fc(x)\n",
    "        return output\n",
    "        \n",
    "    # def get_embedding(self, x):\n",
    "        # return self.forward(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/torch8019/lib/python3.7/site-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
      "/opt/anaconda3/envs/torch8019/lib/python3.7/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EmbeddingNet(\n",
      "  (features): VGG(\n",
      "    (features): Sequential(\n",
      "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (3): ReLU(inplace=True)\n",
      "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (6): ReLU(inplace=True)\n",
      "      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (8): ReLU(inplace=True)\n",
      "      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (11): ReLU(inplace=True)\n",
      "      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (13): ReLU(inplace=True)\n",
      "      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (15): ReLU(inplace=True)\n",
      "      (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (18): ReLU(inplace=True)\n",
      "      (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (20): ReLU(inplace=True)\n",
      "      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (22): ReLU(inplace=True)\n",
      "      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (25): ReLU(inplace=True)\n",
      "      (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (27): ReLU(inplace=True)\n",
      "      (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (29): ReLU(inplace=True)\n",
      "      (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "    (classifier): Sequential()\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=512, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=128, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=128, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(EmbeddingNet())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define siamesenet which have 2 inputs\n",
    "class SiameseNet(nn.Module):\n",
    "    def __init__(self, EmbeddingNet):\n",
    "        super(SiameseNet, self).__init__()\n",
    "        self.EmbeddingNet = EmbeddingNet\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        output1 = self.EmbeddingNet(x1)\n",
    "        output2 = self.EmbeddingNet(x2)\n",
    "        return output1, output2\n",
    "\n",
    "    # def Siamese(self, x1, x2):\n",
    "    #     return self.SiameseNet(x1, x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_net = EmbeddingNet()\n",
    "siamese_net = SiameseNet(embedding_net)\n",
    "print(siamese_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25\n",
      "1\n",
      "2\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "#contrastive loss formula y(d**2) + (1-y)*max(margin-d,0)**2\n",
    "#distance 0.5 similar label=1\n",
    "print(1*(0.5**2) + (1-1)*max((2-0.5),0)**2) #0.25\n",
    "#distance 1 dissimilar\n",
    "print(0*(1**2) + (1-0)*max((2-1),0)**2) #1\n",
    "\n",
    "#因為0是搭的 1是不搭的所以改成這樣\n",
    "print(1*(1**2) + (1*1)*max((2-1),0)**2) #2\n",
    "\n",
    "print(0*(0.5**2) + (1*0)*max((2-0.5),0)**2) #0\n",
    "def closs(d,y,margin):\n",
    "    loss = 1*(d**2) + (1-y)*max((margin-d),0)**2\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(1*(0.5**2) + (1-1)*max((2-0.5),0)**2) #2\n",
    "\n",
    "print(0*(3**2) + (1-0)*max((3-2),0)**2) #0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (3052328748.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/5b/gmxdy6dd0nb181cmtr2vfwrc0000gn/T/ipykernel_77752/3052328748.py\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    super(ContrastiveLoss, self).__init__()\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "#define contrastive loss\n",
    "#loss function reference https://jdhao.github.io/2017/03/13/some_loss_and_explanations/\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=2): #margin要自己定義 2\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        #self.eps = 1e-9\n",
    "    \n",
    "    #這邊可以改成不同距離計算方式 這裡先用歐幾里德距離計算\n",
    "    def forward(self, embedding1, embedding2, label):\n",
    "        distance = F.pairwise_distance(embedding1, embedding2, p=2)\n",
    "        #本來最前面是(1-label)\n",
    "        loss = torch.mean(label * torch.pow(distance, 2) + (1-label) * torch.pow(torch.clamp(self.margin - distance, min=0.0), 2))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training \n",
    "learning_rate = 1e-5\n",
    "epoch = 50\n",
    "batch_size = 32\n",
    "\n",
    "bar_len = []\n",
    "for batch in train_pairs_loader(train_pairs_list,batch_size):\n",
    "    bar_len.append(batch)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    siamese_net.cuda()\n",
    "params = [{'params': md.parameters()} for md in siamese_net.children()\n",
    "          if md in [siamese_net.EmbeddingNet.fc]]\n",
    "optimizer = optim.Adam(siamese_net.parameters(), lr=learning_rate)\n",
    "loss_func = ContrastiveLoss()\n",
    "\n",
    "train_dataloader = train_pairs_loader(train_pairs_list, batch_size)\n",
    "siamese_net.train()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize([224,224]),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[.5,.5,.5],std=[.5,.5,.5])\n",
    "])\n",
    "\n",
    "# threshold = 0.5\n",
    "for i in range(epoch):\n",
    "    print(f'epoch {i+1}')\n",
    "    contrastive_loss = 0.\n",
    "    pbar=tqdm(total=len(bar_len))\n",
    "    train_loss = 0.\n",
    "    train_d_list = []\n",
    "    train_label_list = []\n",
    "    # train_minmax_list = []\n",
    "    # train_ans_list = []\n",
    "    for batch_ in train_pairs_loader(train_pairs_list, batch_size):\n",
    "        for img1_p, img2_p, label_ in batch_ :\n",
    "            img1 = Image.open(img1_p)\n",
    "            img2 = Image.open(img2_p)\n",
    "            img1_t = transform(img1)\n",
    "            img2_t = transform(img2)\n",
    "            x1 = Variable(img1_t).cuda()\n",
    "            x2 = Variable(img2_t).cuda()\n",
    "            label_ = torch.tensor(label_,dtype=torch.int8)\n",
    "            label = Variable(label_).cuda()\n",
    "            optimizer.zero_grad()\n",
    "            embedding1, embedding2 = siamese_net(x1, x2)\n",
    "            loss = loss_func(embedding1, embedding2, label)\n",
    "            train_loss += loss.item()\n",
    "            # train_d_list.append(loss.item())\n",
    "            # train_label_list.append(label)\n",
    "            # contrastive_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            pbar.update(1)\n",
    "        pbar.close()\n",
    "\n",
    "    # big = max(train_d_list)\n",
    "    # small = min(train_d_list)\n",
    "    # correct=0\n",
    "    # for idx,j in enumerate(train_d_list):\n",
    "    #     if j == big:\n",
    "    #         j_nor = 1\n",
    "    #         # train_minmax_list.append(j_nor)\n",
    "    #     elif j == small:\n",
    "    #         j_nor = 0\n",
    "    #         # train_minmax_list.append(j_nor)\n",
    "    #     j_nor = (j-small)/(big-small)\n",
    "    #     # train_minmax_list.append(j_nor)\n",
    "    #     if j_nor < threshold:\n",
    "    #         ans = 0\n",
    "    #     else:\n",
    "    #         ans = 1\n",
    "        \n",
    "    #     if ans == train_label_list[idx]:\n",
    "    #         correct += 1\n",
    "    # # for idx,ans in enumerate(train_ans_list):\n",
    "    # #     if ans == train_label_list[idx]:\n",
    "    # #         correct += 1\n",
    "    # print('Train Loss: {:.6f}, Acc: {:.6f}'.format(train_loss / (len(train_datasets)), correct / (len(train_datasets))))\n",
    "    print('Train Loss: {:.6f}'.format(train_loss / (len(train_pairs_list))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(EmbeddingNet.state_dict(), \"best.pt\")\n",
    "torch.savel(EmbeddingNet, \"best.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch8019",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
